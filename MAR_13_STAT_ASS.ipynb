{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b526b3f-1550-4678-8891-10296f134aa0",
   "metadata": {},
   "source": [
    "## 13 MARCH STATISTICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb546222-8e8e-49d5-9c72-b61b0aecf7a1",
   "metadata": {},
   "source": [
    "## Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results.\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd234e5f-e2fb-4332-b19e-a12185c30e06",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) has several assumptions that should be met in order to ensure the validity of the results. These assumptions include:\n",
    "\n",
    "1. **Homogeneity of variance**: The variances of the groups being compared should be approximately equal. Violations of this assumption, known as heterogeneity of variance or inhomogeneity of variance, can impact the validity of ANOVA results. For example, if one group has a significantly larger variance than the others, it may dominate the analysis and lead to incorrect conclusions.\n",
    "\n",
    "2. **Independence of observations**: The observations within each group should be independent of each other. Violations of this assumption can occur when there is dependence or correlation between observations. For instance, if repeated measurements are taken on the same subjects, or if there is clustering of observations within certain groups, the independence assumption may be violated. This can lead to inflated Type I error rates or incorrect inferences.\n",
    "\n",
    "3. **Normality of residuals**: The residuals (the differences between the observed values and the expected values) should follow a normal distribution. This assumption is required for the validity of hypothesis tests and confidence intervals. Departures from normality can affect the accuracy of p-values and confidence intervals. However, ANOVA is generally robust to moderate departures from normality, especially when sample sizes are large.\n",
    "\n",
    "Examples of violations that could impact the validity of ANOVA results:\n",
    "\n",
    "- **Unequal variances**: If the assumption of homogeneity of variance is violated, the F-statistic may become unreliable. For instance, if one group has a significantly larger variance than the others, it can inflate the F-value, leading to a higher probability of falsely rejecting the null hypothesis.\n",
    "\n",
    "- **Dependence or clustering**: If the observations within groups are not independent, such as in clustered or hierarchical data, the assumption of independence is violated. This can lead to incorrect standard errors, inflated Type I error rates, and biased estimates.\n",
    "\n",
    "- **Non-normal residuals**: If the residuals do not follow a normal distribution, the validity of p-values and confidence intervals may be compromised. However, as mentioned earlier, ANOVA is often robust to moderate departures from normality, especially with larger sample sizes. Nevertheless, severe deviations from normality can affect the accuracy of the results.\n",
    "\n",
    "It is important to assess these assumptions before conducting ANOVA. Diagnostic plots, such as residual plots and Q-Q plots, can help identify violations. In cases where the assumptions are significantly violated, alternative analysis methods or transformations of the data may be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a572d-c735-4dcb-97d1-6e1c29d8b324",
   "metadata": {},
   "source": [
    "## Q2. What are the three types of ANOVA, and in what situations would each be used?\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3437f18a-7a7d-45f7-adde-55c44f33c51f",
   "metadata": {},
   "source": [
    "The three main types of ANOVA are One-Way ANOVA, Two-Way ANOVA, and Three-Way ANOVA. Each type is used in specific situations:\n",
    "\n",
    "1. **One-Way ANOVA**: One-Way ANOVA is used when there is only one independent variable or factor being tested. It is used to compare the means of three or more groups. This type of ANOVA is suitable when there is a single categorical independent variable and a continuous dependent variable. One-Way ANOVA helps determine if there are statistically significant differences among the means of the groups.\n",
    "\n",
    "2. **Two-Way ANOVA**: Two-Way ANOVA is used when there are two independent variables or factors being tested simultaneously. It examines the effects of these two factors on the dependent variable and their interaction. This type of ANOVA is useful when you want to investigate the main effects of each factor individually as well as their combined effect. Two-Way ANOVA can provide insights into how different factors influence the dependent variable and whether there is an interaction effect between the factors.\n",
    "\n",
    "3. **Three-Way ANOVA**: Three-Way ANOVA is an extension of Two-Way ANOVA and is used when there are three independent variables or factors being examined simultaneously. It allows for the analysis of the main effects of each factor, as well as their interactions, including higher-order interactions. Three-Way ANOVA is used when you want to explore the effects of three factors on the dependent variable and understand their individual and combined impacts.\n",
    "\n",
    "The choice of which type of ANOVA to use depends on the research question, the design of the study, and the number of factors being investigated. If there is only one factor, One-Way ANOVA is appropriate. If there are two factors of interest, Two-Way ANOVA is suitable. If there are three factors to be analyzed simultaneously, Three-Way ANOVA is utilized. The goal is to select the ANOVA design that aligns with the research objectives and the complexity of the study design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c75ab-465f-498a-b2a4-4b291d8c887b",
   "metadata": {},
   "source": [
    "## Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec7dae9-afec-485f-aa8c-a13da069492c",
   "metadata": {},
   "source": [
    "The partitioning of variance in ANOVA refers to the division of the total variance observed in the data into different components that can be attributed to specific sources or factors. It is an essential concept in ANOVA as it helps to quantify and understand the relative contributions of different factors to the total variation observed in the data.\n",
    "\n",
    "In ANOVA, the total variance in the data is divided into two main components: variance between groups and variance within groups.\n",
    "\n",
    "1. **Variance between groups**: This component of variance captures the variation that can be attributed to the differences between the group means. It represents the variation explained by the factor or factors being tested in ANOVA. The larger the between-groups variance, the more significant the differences between the groups are considered.\n",
    "\n",
    "2. **Variance within groups**: This component of variance represents the variation that cannot be attributed to the differences between the groups. It reflects the variability or noise within each group that is not accounted for by the factor or factors under investigation.\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "1. **Assessing the significance of the factor**: By partitioning the total variance into between-groups and within-groups components, ANOVA determines if the observed differences between the groups are statistically significant. The F-statistic calculated in ANOVA is based on the ratio of between-groups variance to within-groups variance. It helps in evaluating whether the differences between the groups are larger than what would be expected due to random variation.\n",
    "\n",
    "2. **Determining the relative importance of factors**: The partitioning of variance allows us to understand the relative contributions of different factors to the overall variation observed in the data. By comparing the magnitudes of between-groups variance and within-groups variance, we can assess the impact of the factor being tested on the dependent variable.\n",
    "\n",
    "3. **Interpreting the results**: Understanding the partitioning of variance helps in interpreting the results of ANOVA. It provides insights into how much of the total variation is explained by the factor or factors being examined. This information is crucial for drawing conclusions, making inferences, and understanding the relationship between the independent variables and the dependent variable.\n",
    "\n",
    "By considering the partitioning of variance, researchers can gain a deeper understanding of the factors influencing their data, assess the significance of these factors, and draw meaningful conclusions from ANOVA analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33947c-fb09-4f93-9fca-b6ea943a5e15",
   "metadata": {},
   "source": [
    "## Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d8def8-d608-4214-9049-70779c7b70db",
   "metadata": {},
   "source": [
    "To calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python, you can utilize the `scipy` library. Here's a step-by-step guide on how to calculate these sums of squares:\n",
    "\n",
    "1. Import the necessary libraries:\n",
    "```python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "```\n",
    "\n",
    "2. Prepare the data for your one-way ANOVA. Let's assume you have a list or array called `data` that contains the response variable values for each group.\n",
    "\n",
    "3. Calculate the mean of the entire dataset:\n",
    "\n",
    "```python\n",
    "\n",
    "grand_mean = np.mean(data)\n",
    "\n",
    "```\n",
    "\n",
    "4. Calculate the total sum of squares (SST):\n",
    "\n",
    "```python\n",
    "\n",
    "sst = np.sum((data - grand_mean) ** 2)\n",
    "\n",
    "```\n",
    "\n",
    "5. Calculate the group means for each group:\n",
    "\n",
    "```python\n",
    "\n",
    "group_means = [np.mean(group) for group in groups]\n",
    "\n",
    "```\n",
    "\n",
    "6. Calculate the explained sum of squares (SSE):\n",
    "\n",
    "```python\n",
    "\n",
    "sse = np.sum([len(group) * (group_mean - grand_mean) ** 2 for group, group_mean in zip(groups, group_means)])\n",
    "\n",
    "```\n",
    "\n",
    "7. Calculate the residual sum of squares (SSR):\n",
    "\n",
    "```python\n",
    "\n",
    "ssr = sst - sse\n",
    "\n",
    "```\n",
    "\n",
    "In the above code, `data` represents the entire dataset, and `groups` represents a list of arrays or lists where each array or list corresponds to a different group in the one-way ANOVA.\n",
    "\n",
    "The SST represents the total variation in the data, SSE represents the variation explained by the group means, and SSR represents the unexplained variation or residuals.\n",
    "\n",
    "By calculating these sums of squares, you can further analyze the results of your one-way ANOVA, such as calculating the mean square values, degrees of freedom, and performing statistical tests to evaluate the significance of the differences between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4da20-ec77-4eda-8e74-925fa8bbf348",
   "metadata": {},
   "source": [
    "## Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0540b1a-a521-4fcc-91dd-6c96e9dcab5d",
   "metadata": {},
   "source": [
    "In a two-way ANOVA, you can calculate the main effects and interaction effects using Python by analyzing the sum of squares (SS) and mean squares (MS) associated with each factor. Here's a step-by-step guide on how to calculate these effects:\n",
    "\n",
    "1. Import the necessary libraries:\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "```\n",
    "\n",
    "2. Prepare the data for your two-way ANOVA. Let's assume you have two independent variables, `factor_A` and `factor_B`, and a response variable `data`.\n",
    "\n",
    "3. Calculate the means for each combination of factor levels:\n",
    "```python\n",
    "mean_matrix = np.zeros((len(factor_A_levels), len(factor_B_levels)))\n",
    "for i, level_A in enumerate(factor_A_levels):\n",
    "    for j, level_B in enumerate(factor_B_levels):\n",
    "        mean_matrix[i, j] = np.mean(data[(factor_A == level_A) & (factor_B == level_B)])\n",
    "```\n",
    "\n",
    "4. Calculate the grand mean of the entire dataset:\n",
    "```python\n",
    "grand_mean = np.mean(data)\n",
    "```\n",
    "\n",
    "5. Calculate the main effects:\n",
    "```python\n",
    "main_effect_A = np.sum((np.mean(mean_matrix, axis=1) - grand_mean) ** 2) * len(factor_B_levels)\n",
    "main_effect_B = np.sum((np.mean(mean_matrix, axis=0) - grand_mean) ** 2) * len(factor_A_levels)\n",
    "```\n",
    "\n",
    "6. Calculate the interaction effect:\n",
    "```python\n",
    "interaction_effect = np.sum((mean_matrix - np.mean(mean_matrix, axis=0) - np.mean(mean_matrix, axis=1) + grand_mean) ** 2)\n",
    "```\n",
    "\n",
    "In the above code, `factor_A` and `factor_B` represent the independent variables with their corresponding levels `factor_A_levels` and `factor_B_levels`. `data` represents the response variable.\n",
    "\n",
    "The main effects represent the variation explained by each independent variable individually, whereas the interaction effect represents the additional variation explained by the combined effect of the two independent variables.\n",
    "\n",
    "By calculating these effects, you can gain insights into the relative importance of each independent variable and whether there is an interaction effect between them in influencing the response variable.\n",
    "\n",
    "It's important to note that these calculations assume a balanced design with equal sample sizes for each combination of factor levels. If your design is unbalanced, you may need to consider weighted means or use alternative methods such as Type III sums of squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310923dd-33da-4224-856c-5f8fa46a6797",
   "metadata": {},
   "source": [
    "## Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4e868-33fd-4b63-b509-e0931766867b",
   "metadata": {},
   "source": [
    "In a one-way ANOVA, the F-statistic and the associated p-value are used to determine whether there are statistically significant differences between the group means. Let's analyze the given values:\n",
    "\n",
    "The F-statistic is 5.23, which represents the ratio of the between-groups variation to the within-groups variation. It quantifies the extent to which the group means differ from each other.\n",
    "\n",
    "The p-value is 0.02, which indicates the probability of obtaining the observed F-statistic (or a more extreme value) if there were no true differences between the groups.\n",
    "\n",
    "To interpret these results:\n",
    "\n",
    "1. Based on the obtained F-statistic of 5.23, it suggests that there are differences between the group means. The larger the F-statistic, the stronger the evidence against the null hypothesis of equal group means.\n",
    "\n",
    "2. The p-value of 0.02 indicates that if the null hypothesis (no differences between the groups) were true, there is a 2% chance of observing an F-statistic as extreme as 5.23 or more extreme. Generally, if the p-value is below a pre-defined significance level (e.g., 0.05), it is considered statistically significant.\n",
    "\n",
    "3. Since the p-value is below the significance level (0.05), we reject the null hypothesis and conclude that there are statistically significant differences between the groups. In other words, the group means are unlikely to be equal, and there is evidence to suggest that at least one group mean differs from the others.\n",
    "\n",
    "It is important to note that while the statistical analysis indicates significant differences between the groups, further investigation and post-hoc tests may be needed to determine which specific group(s) differ from each other.\n",
    "\n",
    "In summary, with an F-statistic of 5.23 and a p-value of 0.02, you can conclude that there are statistically significant differences between the groups in the study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a95d9d-ca2d-49f5-ba2a-bf32800cc96a",
   "metadata": {},
   "source": [
    "## Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893fc98b-c4c5-4124-9919-57a9baa49e18",
   "metadata": {},
   "source": [
    "Handling missing data in a repeated measures ANOVA is an important consideration to ensure accurate and unbiased results. There are several methods to handle missing data, and the choice of method can have consequences on the validity of the analysis. Here are some common approaches and their potential consequences:\n",
    "\n",
    "1. **Complete Case Analysis (Listwise deletion)**: This method involves excluding participants with any missing data from the analysis. The consequence of complete case analysis is a potential loss of statistical power and biased estimates if the missingness is related to the variables being analyzed. It may also lead to a reduction in the representativeness of the sample.\n",
    "\n",
    "2. **Mean Imputation**: In mean imputation, missing values are replaced with the mean of the observed values for that variable. The consequence of mean imputation is that it underestimates the variability in the data, potentially distorting the estimates of means and standard errors. It assumes that the missing values are missing completely at random (MCAR) and can introduce bias if data are not MCAR.\n",
    "\n",
    "3. **Last Observation Carried Forward (LOCF)**: LOCF imputes missing values with the value from the last observed timepoint. This method assumes that the missing data remain constant over time. However, LOCF can introduce bias if the missing values are not stable over time or if there are substantial changes between timepoints.\n",
    "\n",
    "4. **Multiple Imputation**: Multiple imputation involves creating multiple plausible imputed datasets, each with a different imputed value for the missing data. The analysis is then conducted on each imputed dataset, and the results are combined using appropriate rules. Multiple imputation provides unbiased estimates if the missing data mechanism is missing at random (MAR) and preserves the uncertainty associated with missing data. However, it requires careful implementation and consideration of the assumptions of MAR.\n",
    "\n",
    "5. **Mixed Effects Models**: Mixed effects models, also known as random effects models or multilevel models, can handle missing data through maximum likelihood estimation. These models use all available data, including partially observed data, and provide unbiased estimates under the assumption of missing at random (MAR). They can handle unbalanced designs and can provide more robust results compared to other methods.\n",
    "\n",
    "When handling missing data in a repeated measures ANOVA, it is important to carefully consider the assumptions of the missing data mechanism, potential biases, and the impact on statistical power. It is recommended to consult with a statistician or utilize specialized software to appropriately handle missing data in the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc443db-bc44-44d4-a8f6-af7e8d1f0bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acd16c88-dc56-44e8-9fc0-5568adf0fb36",
   "metadata": {},
   "source": [
    "## Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary.\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47c94d-2e69-4c05-a0e8-6718c10d8fec",
   "metadata": {},
   "source": [
    "After conducting an ANOVA and finding a significant overall effect, post-hoc tests are used to make specific pairwise comparisons between groups to determine which groups differ significantly from each other. Here are some common post-hoc tests and situations where they are used:\n",
    "\n",
    "1. **Tukey's Honestly Significant Difference (HSD)**: Tukey's HSD test is widely used when performing multiple pairwise comparisons. It controls the family-wise error rate, meaning it maintains the overall Type I error rate at a desired level. It is suitable when the number of pairwise comparisons is moderate.\n",
    "\n",
    "2. **Bonferroni correction**: The Bonferroni correction is a conservative approach to adjust p-values for multiple comparisons. It divides the desired significance level (e.g., 0.05) by the number of pairwise comparisons to determine a more stringent threshold. It is suitable when the number of comparisons is high and you want to control the overall Type I error rate strictly.\n",
    "\n",
    "3. **Dunn's test**: Dunn's test, or the Dunn-Bonferroni post-hoc test, is a non-parametric test used when the assumptions of normality and equal variances are violated. It performs pairwise comparisons using rank-based procedures and adjusts p-values for multiple comparisons.\n",
    "\n",
    "4. **Scheffe's test**: Scheffe's test is a conservative post-hoc test that provides protection against all possible pairwise comparisons, even if they were not initially planned. It is suitable when conducting a large number of comparisons or when there is a priori interest in all pairwise comparisons.\n",
    "\n",
    "5. **Fisher's Least Significant Difference (LSD)**: Fisher's LSD test is an older and less commonly used post-hoc test. It compares group means and calculates the least significant difference for pairwise comparisons. It is less conservative than other tests and can be used when conducting a small number of pairwise comparisons.\n",
    "\n",
    "Example situation: Let's say you conducted a one-way ANOVA to compare the effectiveness of three different treatments on reducing pain levels in patients. The ANOVA results show a statistically significant difference among the treatment groups. In this case, a post-hoc test would be necessary to determine which specific treatments differ significantly from each other. You could apply Tukey's HSD test or another appropriate post-hoc test to make pairwise comparisons and identify the significant differences between the treatment groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf728688-6a33-4ba6-935e-39efded82470",
   "metadata": {},
   "source": [
    "## Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results.\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eea2b7f5-fe8d-4602-8f9a-72ed65bb5afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 335.66110183639313\n",
      "p-value: 3.1739060317598542e-37\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Weight loss data for the three diets\n",
    "diet_A = [2.5, 3.1, 1.8, 2.9, 2.3, 2.6, 2.2, 2.1, 1.9, 2.8, 2.4, 2.7, 2.5, 2.4, 2.6, 2.2, 2.1, 2.0, 1.9, 2.3, 2.7, 2.6, 2.8, 2.4, 2.5]\n",
    "diet_B = [3.5, 3.9, 3.1, 4.0, 3.6, 3.8, 3.7, 3.4, 3.2, 3.5, 3.6, 3.3, 3.5, 3.9, 3.7, 3.4, 3.8, 3.6, 3.9, 3.2, 3.4, 3.6, 3.8, 3.9, 3.5]\n",
    "diet_C = [4.5, 4.1, 4.8, 4.2, 4.6, 4.3, 4.7, 4.4, 4.9, 4.1, 4.3, 4.6, 4.5, 4.2, 4.4, 4.8, 4.6, 4.3, 4.9, 4.7, 4.5, 4.2, 4.1, 4.8, 4.4]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "# Print the results\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334bbac-1af2-408f-a515-e9f489b9b03a",
   "metadata": {},
   "source": [
    "## Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results.\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "799e8a36-353d-4824-9ed2-0ad854827020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       df      sum_sq     mean_sq      F        PR(>F)\n",
      "Software              2.0  606.666667  303.333333  364.0  4.153142e-42\n",
      "Experience            1.0  180.000000  180.000000  216.0  6.145203e-25\n",
      "Software:Experience   2.0    3.333333    1.666667    2.0  1.417287e-01\n",
      "Residual             84.0   70.000000    0.833333    NaN           NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Time data for the three software programs and employee experience levels\n",
    "time_data = pd.DataFrame({\n",
    "    'Software': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'] * 10,\n",
    "    'Experience': ['Novice', 'Novice', 'Experienced'] * 30,\n",
    "    'Time': [12, 15, 10, 18, 20, 16, 14, 13, 11] * 10\n",
    "})\n",
    "\n",
    "# Perform two-way ANOVA\n",
    "model = ols('Time ~ Software + Experience + Software:Experience', data=time_data).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ba4c0e-1d69-41f7-960c-b7fdb51f21d8",
   "metadata": {},
   "source": [
    "## Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other.\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef5a2a4-ac2d-4be5-9f1d-52b3556ca8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -1.7296516067233563\n",
      "p-value: 0.1008029424108484\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Test scores for the control group and experimental group\n",
    "control_group = [85, 90, 75, 80, 95, 78, 92, 88, 82, 87]\n",
    "experimental_group = [92, 88, 94, 85, 91, 83, 89, 86, 90, 95]\n",
    "\n",
    "# Perform two-sample t-test\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "# Print the results\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "print(\"p-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cc26bf-e986-4ff4-afea-a8b0d3df1bb5",
   "metadata": {},
   "source": [
    "## Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post- hoc test to determine which store(s) differ significantly from each other.\n",
    "## ANSWER:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a50f183b-a396-4614-8bfc-0c50cbbb908a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            df       sum_sq    mean_sq         F    PR(>F)\n",
      "Store      2.0     2.488889   1.244444  0.018263  0.981909\n",
      "C(Day)    29.0  1722.888889  59.409962  0.871868  0.649709\n",
      "Residual  58.0  3952.177778  68.140996       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sales data for Store A, Store B, and Store C\n",
    "sales_data = pd.DataFrame({\n",
    "    'Day': np.arange(1, 31),\n",
    "    'Store_A': [100, 110, 95, 105, 90, 115, 100, 105, 92, 110, 115, 100, 105, 93, 105, 110, 95, 105, 90, 115, 100, 105, 92, 110, 115, 100, 105, 93, 105, 110],\n",
    "    'Store_B': [95, 105, 100, 110, 92, 115, 105, 100, 95, 110, 115, 92, 100, 90, 110, 105, 100, 110, 92, 115, 105, 100, 95, 110, 115, 92, 100, 90, 110, 105],\n",
    "    'Store_C': [110, 92, 115, 100, 105, 93, 105, 110, 95, 105, 90, 115, 100, 105, 92, 110, 115, 100, 105, 93, 105, 110, 95, 105, 90, 115, 100, 105, 92, 110]\n",
    "})\n",
    "\n",
    "# Reshape the data to long format\n",
    "sales_data_long = pd.melt(sales_data, id_vars='Day', value_vars=['Store_A', 'Store_B', 'Store_C'], var_name='Store', value_name='Sales')\n",
    "\n",
    "# Perform repeated measures ANOVA\n",
    "model = ols('Sales ~ Store + C(Day)', data=sales_data_long).fit()\n",
    "anova_table = sm.stats.anova_lm(model)\n",
    "\n",
    "# Print the ANOVA table\n",
    "print(anova_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef2e960-9387-45e6-9cb7-5fd2da7293f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
